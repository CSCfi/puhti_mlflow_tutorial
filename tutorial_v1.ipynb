{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for using MLflow on Puhti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through using MLflow in the Puhti computing environment, offering a streamlined and centralized approach to tracking machine learning experiments. It’s tailored for machine learning practitioners who seek an efficient way to manage and monitor their experiments.\n",
    "\n",
    "While prior experience with MLflow isn’t necessary, a basic understanding of supercomputing is recommended. We’ll explore the core components of MLflow and demonstrate their application through practical examples. You can follow along with the provided sample code or incorporate your own code into the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is MLflow?\n",
    "\n",
    "**MLflow** in an open-source tool for managing machine learning models throughout their life cycle. It has four key components that can be widely utilized, from experimenting to deploying models:\n",
    "\n",
    "- **Tracking Server** is the core component used for tracking experiments. Results can be viewed and compared through an informative user interface or API.\n",
    "\t\n",
    "- **Models** is for packaging the models in a unified format, making it easy to move and share them.\n",
    "\n",
    "- **Model Registry** provides tools for registering and versioning models. The registry can also be managed through the UI.\n",
    "\n",
    "- **Projects** is for packaging entire ML project code, enabling easy sharing and reproducibility.\n",
    "\n",
    "By organizing your work into ***experiments*** and ***runs***, MLflow ensures that you can systematically track progress, compare results, and refine your models effectively.\n",
    "\n",
    "For more info on components visit [MLflow documentation](https://mlflow.org/docs/latest/introduction/index.html#what-is-mlflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow, version 2.15.1\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure we have the latest version of MLflow (>2.15.1) in use. If not, run pip install below:\n",
    "!mlflow --version\n",
    "#!pip install --upgrade mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tracking Server & Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Storing Artifacts and Metadata\n",
    "\n",
    "In the example code, we use local storage for both metadata and artifacts. Additionally, MLflow supports various options for selecting the backend and artifact storage.\n",
    "\n",
    "- **Backend Store**: This is where ***metadata*** is stored, including information about each run. By default, MLflow saves this metadata locally in the `mlruns` directory. However, it can be configured to use an external database, such as PostgreSQL or MySQL. You can read more about backend stores here: [MLflow Backend Stores](https://mlflow.org/docs/latest/tracking/backend-stores.html#backend-stores).\n",
    "\n",
    "- **Artifact Store**: This is where the ***artifacts*** generated during runs—such as model weights, model files, and data files—are stored. The **Models component** is used to package these model files in a standardized format. Similar to the backend store, MLflow defaults to using the local `mlruns` directory for artifact storage, but it can be set to use external storage, such as S3 object storage like Allas. For more information, refer to [MLflow Artifact Stores](https://mlflow.org/docs/latest/tracking/artifacts-stores.html) and the [CSC documentation on Allas](https://docs.csc.fi/computing/allas/) and [using Allas with Python and Boto3](https://docs.csc.fi/data/Allas/using_allas/python_boto3/).\n",
    "\n",
    "In the example code below, local directory is used for the backend and artifact storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts are saved stored here: /scratch/dac/esternad/mlruns/645056726564230381\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "First, we activate the Tracking Server component:\n",
    "\n",
    "Start by setting the tracking URI, which defines the path where MLflow will create the 'mlruns' directory to store metadata and artifacts generated during runs. \n",
    "If no path is provided, MLflow will create the 'mlruns' directory in the location where the code is executed.\n",
    "\n",
    "Next, set up an experiment under which the upcoming training runs will be logged. If the experiment does not already exist, it will be created.\n",
    "\"\"\"\n",
    "\n",
    "# Set tracking URI\n",
    "project_id = \"dac\" \n",
    "#project_id = \"project_2001234\" # Insert your project_id here\n",
    "mlruns_uri = f\"/scratch/{project_id}/esternad/mlruns\" \n",
    "#mlruns_uri = f\"scratch/{project_id]/path/for/mlruns\" # URI for desired storage\n",
    "mlflow.set_tracking_uri(mlruns_uri) # https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow-tracing-apis\n",
    "\n",
    "# Set experiment\n",
    "experiment_name = \"MLflow tutorial\"\n",
    "experiment = mlflow.set_experiment(experiment_name) # https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow-tracing-apis\n",
    "print(f\"Artifacts are saved stored here: {experiment.artifact_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: sequential_3layers\n",
      "Epoch 1/5\n",
      "    1/60000 [..............................] - ETA: 7:13:00 - loss: 2.0011 - accuracy: 0.0000e+00WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0018s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0018s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 124s 2ms/step - loss: 0.2099 - accuracy: 0.9392 - val_loss: 0.1408 - val_accuracy: 0.9592\n",
      "Epoch 2/5\n",
      "   83/60000 [..............................] - ETA: 1:52 - loss: 0.1127 - accuracy: 0.9639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 126s 2ms/step - loss: 0.1259 - accuracy: 0.9679 - val_loss: 0.1318 - val_accuracy: 0.9695\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 0.1058 - accuracy: 0.9746 - val_loss: 0.1657 - val_accuracy: 0.9668\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 0.0934 - accuracy: 0.9780 - val_loss: 0.1629 - val_accuracy: 0.9706\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 125s 2ms/step - loss: 0.0836 - accuracy: 0.9806 - val_loss: 0.1732 - val_accuracy: 0.9716\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/esternad/22951873/tmpp478ag7k/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/esternad/22951873/tmpp478ag7k/model/data/model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.1732 - accuracy: 0.9716 - 499ms/epoch - 2ms/step\n",
      "\n",
      "Test accuracy: 0.9715999960899353\n",
      "Run name: sequential_with_tanh\n",
      "Epoch 1/5\n",
      "    1/60000 [..............................] - ETA: 10:44:49 - loss: 2.5397 - accuracy: 0.0000e+00WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.2455 - accuracy: 0.9253 - val_loss: 0.1912 - val_accuracy: 0.9421\n",
      "Epoch 2/5\n",
      "   77/60000 [..............................] - ETA: 2:00 - loss: 0.0748 - accuracy: 0.9870"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 131s 2ms/step - loss: 0.1488 - accuracy: 0.9564 - val_loss: 0.1630 - val_accuracy: 0.9509\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 134s 2ms/step - loss: 0.1327 - accuracy: 0.9601 - val_loss: 0.1565 - val_accuracy: 0.9553\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 135s 2ms/step - loss: 0.1187 - accuracy: 0.9639 - val_loss: 0.1192 - val_accuracy: 0.9656\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 135s 2ms/step - loss: 0.1121 - accuracy: 0.9661 - val_loss: 0.1335 - val_accuracy: 0.9638\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/esternad/22951873/tmpe097gz96/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/esternad/22951873/tmpe097gz96/model/data/model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.1335 - accuracy: 0.9638 - 531ms/epoch - 2ms/step\n",
      "\n",
      "Test accuracy: 0.9638000130653381\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Next, we will run training sessions using MLflow. We'll utilize the autolog function, which automatically logs all the data generated during the run. \n",
    "By default, the model will be logged as an artifact, making it easy to access later on. This and other features can be modified in arguments.\n",
    "In the example code, we perform two training rounds with slightly different models, allowing us to compare the results in the UI.   \n",
    "\"\"\"\n",
    "\n",
    "mlflow.autolog() # https://mlflow.org/docs/latest/tracking/autolog.html#automatic-logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Let's take two slightly different models to compare. You can either use the example code or insert your own.\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255.\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "model_1 = Sequential(\n",
    "    [\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model_1.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_2 = Sequential(\n",
    "    [\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation='tanh'),\n",
    "        layers.Dense(64, activation='tanh'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model_2.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "models = {model_1: \"sequential_3layers\", \n",
    "          model_2: \"sequential_with_tanh\"}\n",
    "\n",
    "for model, name in models.items():\n",
    "        \n",
    "    with mlflow.start_run(): # Trigger MLflow to start tracking the run: https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=autolog#mlflow.start_run\n",
    "        \n",
    "        run_name = name # Assign an informative name to the run; otherwise, a random name will be generated.\n",
    "        mlflow.set_tag(\"mlflow.runName\", run_name) \n",
    "        print(f\"Run name: {run_name}\")\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=5, batch_size=1, validation_data=(X_test, y_test))\n",
    "\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "        print('\\nTest accuracy:', test_acc)\n",
    "        \n",
    "        mlflow.end_run() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Interface\n",
    "\n",
    "Next, we'll launch the MLflow application on Puhti and explore its user interface. You can find the MLflow icon under the Apps menu in Puhti. Ensure that the `pytorch/2.4` module is selected in the Settings section, as it includes a sufficiently recent version of MLflow. Additionally, make sure the file path points to the previously specified `mlruns` directory.\n",
    "\n",
    "![mlflow_puhti](./pics/mlflow_puhti.png)\n",
    "\n",
    "Picture 1. Setup MLflow in Puhti\n",
    "\n",
    "After launching successfully, we can view the previous runs:\n",
    "\n",
    "![run_1.png](./pics/runs_1.png)\n",
    "\n",
    "Picture 2. Front page of MLflow UI\n",
    "\n",
    "**Image caption:**\n",
    "\n",
    "1. All experiments are listed here. With informative names and optional descriptions, user can organize different runs into easily manageable collections.\n",
    "2. When selecting an experiment, all associated runs are displayed. User can sort and group these runs in various ways.\n",
    "\n",
    "In the Charts view (see picture below), users can compare the performance of different models using automatically generated graphs. These graphs can be downloaded in formats like CSV. \n",
    "\n",
    "![run_2.png](./pics/run_2.png)\n",
    "\n",
    "Picture 3. Charts- view of runs\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The Tracking Server and Models components together provide an easy way to centrally monitor and store machine learning experiments in a consistent manner. These tools are user-friendly and don’t require deep expertise to get started. However, MLflow also offers the flexibility for more detailed configuration to meet the needs of more demanding use cases.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
