{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for using MLflow on Puhti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through using MLflow in the Puhti computing environment, offering a streamlined and centralized approach to tracking machine learning experiments. It’s tailored for machine learning practitioners who seek an efficient way to manage and monitor their experiments.\n",
    "\n",
    "While prior experience with MLflow isn’t necessary, a basic understanding of supercomputing is recommended. We’ll explore the core components of MLflow and demonstrate their application through practical examples. You can follow along with the provided sample code or incorporate your own code into the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is MLflow?\n",
    "\n",
    "**MLflow** in an open-source tool for managing machine learning models throughout their life cycle. It has four key components that can be widely utilized, from experimenting to deploying models:\n",
    "\n",
    "- **Tracking Server** is the core component used for tracking experiments. Results can be viewed and compared through an informative user interface or API.\n",
    "\t\n",
    "- **Models** is for packaging the models in a unified format, making it easy to move and share them.\n",
    "\n",
    "- **Model Registry** provides tools for registering and versioning models. The registry can also be managed through the UI.\n",
    "\n",
    "- **Projects** is for packaging entire ML project code, enabling easy sharing and reproducibility.\n",
    "\n",
    "By organizing your work into ***experiments*** and ***runs***, MLflow ensures that you can systematically track progress, compare results, and refine your models effectively.\n",
    "\n",
    "For more info on components visit [MLflow documentation](https://mlflow.org/docs/latest/introduction/index.html#what-is-mlflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's make sure we have the latest version of MLflow (>2.15.1) in use. If not, run pip install below:\n",
    "!mlflow --version\n",
    "#!pip install --upgrade mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## MLflow Tracking Server & Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Storing Artifacts and Metadata\n",
    "\n",
    "In the example code, we use local storage for both metadata and artifacts. Additionally, MLflow supports various options for selecting the backend and artifact storage.\n",
    "\n",
    "- **Backend Store**: This is where ***metadata*** is stored, including information about each run. By default, MLflow saves this metadata locally in the `mlruns` directory. However, it can be configured to use an external database, such as PostgreSQL or MySQL. You can read more about backend stores here: [MLflow Backend Stores](https://mlflow.org/docs/latest/tracking/backend-stores.html#backend-stores).\n",
    "\n",
    "- **Artifact Store**: This is where the ***artifacts*** generated during runs—such as model weights, model files, and data files—are stored. The **Models component** is used to package these model files in a standardized format. Similar to the backend store, MLflow defaults to using the local `mlruns` directory for artifact storage, but it can be set to use external storage, such as S3 object storage like Allas. For more information, refer to [MLflow Artifact Stores](https://mlflow.org/docs/latest/tracking/artifacts-stores.html) and the [CSC documentation on Allas](https://docs.csc.fi/computing/allas/) and [using Allas with Python and Boto3](https://docs.csc.fi/data/Allas/using_allas/python_boto3/).\n",
    "\n",
    "In the example code below, local directory is used for the backend and artifact storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: unmatched ']' (3660827531.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    mlruns_uri = f\"scratch/{project_id]/path/to/mlruns\" # URI for desired storage\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: unmatched ']'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "First, we activate the Tracking Server component:\n",
    "\n",
    "Start by setting the tracking URI, which defines the path where MLflow will create the 'mlruns' directory to store metadata and artifacts generated during runs. If no path is provided, MLflow will create the 'mlruns' directory in the location where the code is executed.\n",
    "\n",
    "Next, set up an experiment under which the upcoming training runs will be logged. If the experiment does not already exist, it will be created.\n",
    "\"\"\"\n",
    "\n",
    "# Set tracking URI\n",
    "project_id = \"project_2001234\" # Insert your project_id here\n",
    "mlruns_uri = f\"scratch/{project_id}/path/to/mlruns\" # URI for desired storage\n",
    "mlflow.set_tracking_uri(mlruns_uri) # https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow-tracing-apis\n",
    "\n",
    "# Set experiment\n",
    "experiment_name = \"MLflow tutorial\"\n",
    "experiment = mlflow.set_experiment(experiment_name) # https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow-tracing-apis\n",
    "print(f\"Artifacts are saved stored here: {experiment.artifact_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next, we will run training sessions using MLflow. We'll utilize the autolog function, which automatically logs all the data generated during the run. By default, the model will be logged as an artifact, making it easy to access later on and enabling automatic versioning. This and other features can be modified in arguments.\n",
    "\n",
    "In the example code, we perform two training rounds with slightly different models, allowing us to compare the results in the UI.   \n",
    "\"\"\"\n",
    "\n",
    "mlflow.autolog() # https://mlflow.org/docs/latest/tracking/autolog.html#automatic-logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Let's compile two slightly different models to compare. You can either use the example code or insert your own.\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255.\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "model_1 = Sequential(\n",
    "    [\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model_1.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_2 = Sequential(\n",
    "    [\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation='tanh'),\n",
    "        layers.Dense(64, activation='tanh'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model_2.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "models = {model_1: \"sequential_3layers\", \n",
    "          model_2: \"sequential_with_tanh\"}\n",
    "\n",
    "for model, name in models.items():\n",
    "    \n",
    "    with mlflow.start_run(): # Trigger MLflow to start tracking the run: https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=autolog#mlflow.start_run\n",
    "        \n",
    "        run_name = name # Assign an informative name to the run; otherwise, a random name will be generated.\n",
    "        mlflow.set_tag(\"mlflow.runName\", run_name) \n",
    "        print(f\"Run name: {run_name}\")\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=5, batch_size=1, validation_data=(X_test, y_test))\n",
    "\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "        print('\\nTest accuracy:', test_acc)\n",
    "        \n",
    "        mlflow.end_run() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Interface\n",
    "\n",
    "Next, we'll launch the MLflow application on Puhti and explore its user interface. You can find the MLflow icon under the Apps menu in Puhti. Ensure that the `pytorch/2.4` module is selected in the Settings section, as it includes a sufficiently recent version of MLflow. Additionally, make sure the file path points to the previously specified `mlruns` directory.\n",
    "\n",
    "![mlflow_puhti.png](./pics/mlflow_puhti.png)\n",
    "\n",
    "<small>Figure 1. Setup MLflow in Puhti\n",
    "\n",
    "After launching successfully, we can view the previous runs:\n",
    "\n",
    "![run_1.png](./pics/runs_1.png)\n",
    "\n",
    "<small>Figure 2. Front page of MLflow UI</small>\n",
    "\n",
    "**Image caption:**\n",
    "\n",
    "1. All experiments are listed here. With informative names and optional descriptions, user can organize different runs into easily manageable collections.\n",
    "2. When selecting an experiment, all associated runs are displayed. User can sort and group these runs in various ways.\n",
    "\n",
    "In the Charts view (Figure 3), users can compare the performance of different models using automatically generated graphs. These graphs can be downloaded in formats like CSV. \n",
    "\n",
    "![run_2.png](./pics/run_2.png)\n",
    "\n",
    "<small>Figure 3. Charts- view of runs</small>\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The Tracking Server and Models components together provide an easy way to centrally monitor and store machine learning experiments in a consistent manner. These tools are user-friendly and don’t require deep expertise to get started. However, MLflow also offers the flexibility for more detailed configuration to meet the needs of more demanding use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## MLflow Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you identify a model through experimentation that is ready for production, you can take advantage of the **[MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html#mlflow-model-registry)**. The registry offers a centralized platform for managing, validating, and deploying models. You can assign aliases such as \"staging\" or \"production\" to your models, making it easy to retrieve the appropriate model from the registry for inference.\n",
    "\n",
    "The Model Registry can be managed through either the Models tab in the user interface or via the [API](https://mlflow.org/docs/latest/model-registry.html#adding-an-mlflow-model-to-the-model-registry).\n",
    "\n",
    "![register_model.png](./pics/register_model.png)\n",
    "\n",
    "<small>Figure 4. Register model in UI</small>\n",
    "\n",
    "**Image caption:**\n",
    "\n",
    "1. By opening the details of the desired run, you can register it from the top left corner.\n",
    "2. Name the new model or select an existing one from the menu. If the model is already registered, a new version will be created.\n",
    "\n",
    "Once the model is registered, you can view and manage it on the Models tab (Figures 5 and 6).\n",
    "\n",
    "![model_reg1.png](./pics/model_reg1.png)\n",
    "\n",
    "<small>Figure 5. Model Registry front page</small>\n",
    "\n",
    "![model_reg2.png](./pics/model_reg2.png)\n",
    "\n",
    "<small>Figure 6. Details of a registered model</small>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To register another trained model using the API, you need the run ID of the model you want to register. Since a previously registered model via the UI had a lower accuracy, we will now programmatically find and register the model with the highest validation_accuracy. Additionally, we will retrieve the model's name using API calls.\n",
    "Finally, we assign the alias \"challenger\" to the model for easier identification. To accomplish this, we use the MLflow Client, which allows us to programmatically manage model aliases and streamline the process.\n",
    "\"\"\"\n",
    "\n",
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# https://mlflow.org/docs/latest/search-runs.html\n",
    "runs = mlflow.search_runs([experiment.experiment_id]) \n",
    "run_id = runs.loc[runs['metrics.accuracy'].idxmax(), 'run_id']\n",
    "\n",
    "# https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.register_model\n",
    "filter_string = \"name LIKE 'mnist%'\"\n",
    "model_uri = f\"runs:/{run_id}\"\n",
    "model_name = (mlflow.search_registered_models(filter_string=filter_string))[0].name\n",
    "mv = mlflow.register_model(model_uri, model_name) \n",
    "\n",
    "# https://mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.client.MlflowClient.set_model_version_tag\n",
    "client.set_registered_model_alias(mv.name, \"challenger\", mv.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When multiple versions of a model are available, they can be easily compared through the user interface.\n",
    "\n",
    "![model_reg3.png](./pics/model_reg3.png)\n",
    "\n",
    "<small>Figure 7. Model versions to compare</small>\n",
    "\n",
    "![model_reg4.png](./pics/model_reg4.png)\n",
    "\n",
    "<small>Figure 7. Comparing registered models</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment and Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m alias \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchampion\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist_sequential\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241m.\u001b[39mpyfunc\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malias\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlflow' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "alias = \"champion\"\n",
    "model_name = \"mnist_sequential\"\n",
    "model = mlflow.pyfunc.load_model(f\"models:{model_name}@{alias}\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tämä onkin mielenkiintoinen sillä en ole itse ennen käyttänyt vaan plaaplaa plaa, miksi käyttäisin ja milloin ja miten ja esimerkkikoodia perään"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
