{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h1>Tutorial for using MLflow on Puhti</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./pics/CSC_2012_LOGO.png\" alt=\"CSC_2012_LOGO.png\" width=\"400\" height=\"300\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through using MLflow in the Puhti computing environment, offering a streamlined and centralized approach to tracking machine learning experiments. It’s tailored for machine learning practitioners who seek an efficient way to manage and monitor their experiments.\n",
    "\n",
    "While prior experience with MLflow isn’t necessary, a basic understanding of supercomputing is recommended. We’ll explore the core components of MLflow and demonstrate their application through practical examples. You can follow along with the provided sample code or incorporate your own code into the tutorial.\n",
    "\n",
    "The sample code creates and trains two neural network models for classifying MNIST digits. **Model 1**, `ReLU_2Layer_Adagrad`, uses ReLU activation with the Adagrad optimizer. **Model 2**, `ReLU_2Layer_Adam`, has the same architecture but uses the Adam optimizer. Their performance is compared using MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is MLflow?\n",
    "\n",
    "**MLflow** in an open-source tool for managing machine learning models throughout their life cycle. It has four key components that can be widely utilized, from experimenting to deploying models:\n",
    "\n",
    "- [**Tracking Server**](https://mlflow.org/docs/latest/tracking.html) is the core component used for tracking experiments. Results can be viewed and compared through an informative user interface or API.\n",
    "\t\n",
    "- [**Models**](https://mlflow.org/docs/latest/models.html) is for packaging the models in a unified format, making it easy to move and share them.\n",
    "\n",
    "- [**Model Registry**](https://mlflow.org/docs/latest/model-registry.html) provides tools for registering and versioning models. The registry can also be managed through the UI.\n",
    "\n",
    "- [**Projects**](https://mlflow.org/docs/latest/projects.html) is for packaging entire ML project code, enabling easy sharing and reproducibility.\n",
    "\n",
    "By organizing your work into ***experiments*** and ***runs***, MLflow ensures that you can systematically track progress, compare results, and refine your models effectively.\n",
    "\n",
    "For more info on components visit [MLflow documentation](https://mlflow.org/docs/latest/introduction/index.html#what-is-mlflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's make sure we have the latest version of MLflow (>2.15.1) in use. If not, run pip install below:\n",
    "!mlflow --version\n",
    "#!pip install --upgrade mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With sample code, we use Grdaio Interface to demo deployment and inference:\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import gradio as gr\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## MLflow Tracking Server & Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Storing Artifacts and Metadata\n",
    "\n",
    "In the example code, we use local storage for both metadata and artifacts. Additionally, MLflow supports various options for selecting the backend and artifact storage.\n",
    "\n",
    "- **Backend Store**: This is where ***metadata*** is stored, including information about each run. By default, MLflow saves this metadata locally in the `mlruns` directory. However, it can be configured to use an external database, such as PostgreSQL or MySQL. You can read more about backend stores here: [MLflow Backend Stores](https://mlflow.org/docs/latest/tracking/backend-stores.html#backend-stores).\n",
    "\n",
    "- **Artifact Store**: This is where the ***artifacts*** generated during runs—such as model weights, model files, and data files—are stored. The **Models component** is used to package these model files in a standardized format. Similar to the backend store, MLflow defaults to using the local `mlruns` directory for artifact storage, but it can be set to use external storage, such as S3 object storage like Allas. For more information, refer to [MLflow Artifact Stores](https://mlflow.org/docs/latest/tracking/artifacts-stores.html) and the [CSC documentation on Allas](https://docs.csc.fi/computing/allas/) and [using Allas with Python and Boto3](https://docs.csc.fi/data/Allas/using_allas/python_boto3/).\n",
    "\n",
    "In the example code below, local directory is used for the backend and artifact storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring MLflow\n",
    "\n",
    "First, we activate the **Tracking Server** component:\n",
    "\n",
    "Start by setting the tracking URI, which defines the path where MLflow will create the `mlruns` directory to store metadata and artifacts generated during runs. \n",
    "If no path is provided, MLflow will create the `mlruns` directory in the location where the code is executed.\n",
    "\n",
    "Next, set up an experiment under which the upcoming training runs will be logged. If the experiment does not already exist, it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tracking URI\n",
    "project_id = \"project_2001234\" # Insert CSC your project_id here\n",
    "mlruns_uri = f\"scratch/{project_id}/path/to/mlruns\" # URI for desired storage, you can decide the location\n",
    "mlflow.set_tracking_uri(mlruns_uri) # https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow-tracing-apis\n",
    "\n",
    "# Set experiment\n",
    "experiment_name = \"MLflow tutorial\"\n",
    "experiment = mlflow.set_experiment(experiment_name) # https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow-tracing-apis\n",
    "print(f\"Artifacts are saved stored here: {experiment.artifact_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with MLflow\n",
    "\n",
    "Next, we will run training sessions using MLflow. We'll utilize the autolog function, which automatically logs all the data generated during the run. \n",
    "By default, the model will be logged as an artifact, making it easy to access later on and enabling automatic versioning. \n",
    "This and other features can be modified in arguments.\n",
    "\n",
    "In the example code, we perform two training rounds with slightly different models, allowing us to compare the results in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlflow.autolog() # https://mlflow.org/docs/latest/tracking/autolog.html#automatic-logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Let's compile two slightly different models to compare. You can either use the example code or insert your own.\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255.\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "model_1 = Sequential(\n",
    "    [\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model_1.compile(optimizer='adagrad', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_2 = Sequential(\n",
    "    [\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model_2.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "models = {model_1: \"3Layer_adagrad\", \n",
    "          model_2: \"3Layer_adam\"}\n",
    "\n",
    "for model, name in models.items():\n",
    "    \n",
    "    with mlflow.start_run(): # Trigger MLflow to start tracking the run: https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=autolog#mlflow.start_run\n",
    "        \n",
    "        run_name = name # Assign an informative name to the run; otherwise, a random name will be generated.\n",
    "        mlflow.set_tag(\"mlflow.runName\", run_name) \n",
    "        print(f\"Run name: {run_name}\")\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "        print('\\nTest accuracy:', test_acc)\n",
    "        \n",
    "        mlflow.end_run() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Interface\n",
    "\n",
    "Next, we'll launch the MLflow application on Puhti and explore its user interface. You can find the MLflow icon under the Apps menu in Puhti. Ensure that the `pytorch/2.4` module is selected in the Settings section, as it includes a sufficiently recent version of MLflow. Additionally, make sure the file path points to the previously specified `mlruns` directory.\n",
    "\n",
    "![mlflow_puhti.png](./pics/mlflow_puhti.png)\n",
    "\n",
    "<small>Figure 1. Setup MLflow in Puhti</small>\n",
    "\n",
    "After launching successfully, we can view the previous runs:\n",
    "\n",
    "![run_1.png](./pics/runs_1.png)\n",
    "\n",
    "<small>Figure 2. Front page of MLflow UI</small>\n",
    "\n",
    "**Image caption:**\n",
    "\n",
    "1. All experiments are listed here. With informative names and optional descriptions, user can organize different runs into easily manageable collections.\n",
    "2. When selecting an experiment, all associated runs are displayed. User can sort and group these runs in various ways.\n",
    "\n",
    "In the Charts view (Figure 3), users can compare the performance of different models using automatically generated graphs. These graphs can be downloaded in formats like CSV. \n",
    "\n",
    "![run_2.png](./pics/run_2.png)\n",
    "\n",
    "<small>Figure 3. Charts- view of runs</small>\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The Tracking Server and Models components together provide an easy way to centrally monitor and store machine learning experiments in a consistent manner. These tools are user-friendly and don’t require deep expertise to get started. However, MLflow also offers the flexibility for more detailed configuration to meet the needs of more demanding use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## MLflow Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you identify a model through experimentation that is ready for production, you can take advantage of the **[MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html#mlflow-model-registry)**. The registry offers a centralized platform for managing, validating, and deploying models. You can assign aliases such as \"champion\" or \"challenger\" to your models, making it easy to retrieve the appropriate model from the registry for inference.\n",
    "\n",
    "The Model Registry can be managed through either the Models tab in the user interface or via the [API](https://mlflow.org/docs/latest/model-registry.html#adding-an-mlflow-model-to-the-model-registry).\n",
    "\n",
    "![register_model.png](./pics/register_model.png)\n",
    "\n",
    "<small>Figure 4. Register model in UI</small>\n",
    "\n",
    "**Image caption:**\n",
    "\n",
    "1. By opening the details of the desired run, you can register it from the top left corner.\n",
    "2. Name the new model or select an existing one from the menu. If the model is already registered, a new version will be created.\n",
    "\n",
    "<p style=\"color: red;\">To complete following examples, register the `3Layer_adagrad` model, or the one with lower `validation_accuracy`.</p>\n",
    "\n",
    "#### MLflow UI Models\n",
    "\n",
    "Once the model is registered, you can view and manage it on the Models tab (Figure 5).\n",
    "\n",
    "![model_reg1.png](./pics/model_reg1.png)\n",
    "\n",
    "<small>Figure 5. Model Registry front page</small>\n",
    "\n",
    "You can also manually add description, tags and alias (Figure 6).\n",
    "<p style=\"color: red;\">To complete following examples, add \"champion\" alias for the model registered before.</p>\n",
    "\n",
    "![model_reg2.png](./pics/model_reg2.png)\n",
    "\n",
    "<small>Figure 6. Details of a registered model</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a model via API\n",
    "\n",
    "To register another trained model using the API, you need the run ID of the model you want to register. \n",
    "\n",
    "Since a previously registered model via the UI had a lower accuracy, we will now programmatically find and register the model with the highest `validation_accuracy`. \n",
    "Additionally, we will retrieve the model's name using API calls.\n",
    "\n",
    "Finally, we assign the alias \"challenger\" to the model for easier identification. To accomplish this, we use the [MLflow Client](https://www.mlflow.org/docs/latest/python_api/mlflow.client.html?highlight=client#module-mlflow.client), which allows us to programmatically manage model aliases and streamline the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# https://mlflow.org/docs/latest/search-runs.html\n",
    "runs = mlflow.search_runs([experiment.experiment_id]) \n",
    "run_id = runs.loc[runs['metrics.validation_accuracy'].idxmax(), 'run_id']\n",
    "\n",
    "# https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.register_model\n",
    "filter_string = \"name LIKE 'mnist%'\"\n",
    "model_uri = f\"runs:/{run_id}\"\n",
    "model_name = (mlflow.search_registered_models(filter_string=filter_string))[0].name\n",
    "mv = mlflow.register_model(model_uri, model_name) \n",
    "\n",
    "# https://mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.client.MlflowClient.set_model_version_tag\n",
    "client.set_registered_model_alias(mv.name, \"challenger\", mv.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When multiple versions of a model are available, they can be easily compared through the user interface (Figures 7 and 8).\n",
    "\n",
    "![model_reg3.png](./pics/model_reg3.png)\n",
    "\n",
    "<small>Figure 7. Model versions to compare</small>\n",
    "\n",
    "![model_reg4.png](./pics/model_reg4.png)\n",
    "\n",
    "<small>Figure 7. Comparing registered models</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment and Inference\n",
    "\n",
    "As previously mentioned, the Model Registry allows for easy deployment of models using aliases.\n",
    "\n",
    "In the following example code, we use the [**MLflow Models**](https://mlflow.org/docs/latest/models.html)- module to retrieve a previously registered model by its alias for deployment. The model is then utilized for inference through a lightweight [Gradio](https://www.gradio.app/) UI. Gradio is a user friendly and fast tool for demoing machine learning models.\n",
    "\n",
    "When loading model versions, *model flavors* are used to ensure the model is loaded in the correct format. The [`pyfunc`](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#module-mlflow.pyfunc) flavor is the default and works with all models stored using MLflow. Additionally, MLflow supports various other [model flavors](https://mlflow.org/docs/latest/python_api/mlflow.models.html?highlight=flavors#module-mlflow.models), such as `sklearn`, `keras`, `pytorch`, and `xgboost`, which are specifically optimized for these libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model\n",
    "\n",
    "alias = \"champion\"\n",
    "model_name = \"mnist_sequential\"\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}@{alias}\") \n",
    "print(model)\n",
    "print()\n",
    "\n",
    "def classify(image):\n",
    "    \n",
    "    \"\"\"\n",
    "    This code processes an image by converting it to a NumPy array, normalizing pixel values (0-1), \n",
    "    and converting it to grayscale if it has 3 or 4 channels (RGB or RGBA). \n",
    "    It then resizes the image to 28x28 pixels and reshapes it into a (1, 28, 28) array.\n",
    "    \"\"\"\n",
    "    \n",
    "    composite_image = image['composite']\n",
    "    composite_image = np.array(composite_image, dtype=np.float64)\n",
    "    composite_image = composite_image / 255.0\n",
    "\n",
    "    if composite_image.shape[-1] == 4:  \n",
    "        composite_image = np.mean(composite_image[..., :3], axis=-1)\n",
    "    elif composite_image.shape[-1] == 3:  \n",
    "        composite_image = np.mean(composite_image, axis=-1)\n",
    "\n",
    "    composite_image = resize(composite_image, (28, 28), anti_aliasing=True)\n",
    "    composite_image = np.reshape(composite_image, (1, 28, 28))\n",
    "\n",
    "    prediction = model.predict(composite_image).tolist()[0]\n",
    "    return {str(i): prediction[i] for i in range(10)}\n",
    "\n",
    "# https://www.gradio.app/docs\n",
    "\n",
    "sketchpad = gr.Sketchpad()\n",
    "label = gr.Label(num_top_classes=10)\n",
    "\n",
    "interface = gr.Interface(fn=classify, inputs=sketchpad, outputs=label, live=False, allow_flagging=\"never\")\n",
    "\n",
    "interface.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Projects (advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MLflow Projects](https://mlflow.org/docs/latest/projects.html) organizes code into **self-contained, reusable components** that can be run on different environments. A project in MLflow is essentially a **directory** or **Git repository** containing:\n",
    "\n",
    "1. **Code (scripts)** for performing machine learning tasks.\n",
    "2. **`MLproject` file**: A YAML configuration file that defines:\n",
    "    - Project name\n",
    "    - Dependencies (e.g., environment, for Puhti see https://docs.csc.fi/apps/python/#pre-installed-python-environments)\n",
    "    - Entry points (main scripts/functions with parameters for tasks like training or evaluation)\n",
    "\n",
    "The `MLproject` file might look like this:\n",
    "\n",
    "```yaml\n",
    "name: my_ml_project \n",
    "entry_points: \n",
    "\tmain: \n",
    "\t\tparameters: learning_rate: {type: float, default: 0.01} \n",
    "\t\tepochs: {type: int, default: 10} \n",
    "\t\tcommand: | \n",
    "\t\t\tmodule load ptensorflow\n",
    "\t\t\tpython train.py --learning_rate {learning_rate} --epochs {epochs}\n",
    "\n",
    "\n",
    "```\n",
    "It would be utilized in a job script in the following manner:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=mlflow_project\n",
    "#SBATCH --output=mlflow_output_%j.log\n",
    "#SBATCH --error=mlflow_error_%j.log\n",
    "#SBATCH --time=00:01:00\n",
    "#SBATCH --partition=test\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --mem-per-cpu=1G\n",
    "#SBATCH --account=project_20001234\n",
    "\n",
    "module load python-data\n",
    "\n",
    "mlflow run . -P learning_rate=0.01 -P epochs=10\n",
    "\n",
    "\n",
    "```\n",
    "For more information, see MLflow documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
